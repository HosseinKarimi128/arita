{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DIRECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.generativeai'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgenerativeai\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mgenai\u001b[39;00m\n\u001b[1;32m      3\u001b[0m genai\u001b[38;5;241m.\u001b[39mconfigure(api_key\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGOOGLE_API_KEY\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      4\u001b[0m myfile \u001b[38;5;241m=\u001b[39m genai\u001b[38;5;241m.\u001b[39mupload_file(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNew Light Gidge.mp3\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google.generativeai'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import google.generativeai as genai\n",
    "genai.configure(api_key=os.environ['GOOGLE_API_KEY'])\n",
    "myfile = genai.upload_file(\"New Light Gidge.mp3\")\n",
    "model = genai.GenerativeModel(\"gemini-1.5-pro\")\n",
    "result = model.generate_content(\n",
    "    [myfile, \"\\n\\n\", \"what is genre of this audio?\"]\n",
    ")\n",
    "print(f\"{result.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LANGCHAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.generativeai'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgenerativeai\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mgenai\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_google_genai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChatGoogleGenerativeAI\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmessages\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HumanMessage\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google.generativeai'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import google.generativeai as genai\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "os.environ['GOOGLE_API_KEY'] = \"AIzaSyD7lveG7AUMh73mP4O53QKMI4ba8Ozk2Qc\"\n",
    "genai.configure(api_key=os.environ['GOOGLE_API_KEY'])\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "_file = genai.upload_file(\"image (3).png\")\n",
    "# _file = genai.upload_file(\"New Light Gidge.mp3\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_message_text = '''\n",
    "What is genre of this artifact and with which genra is blended,\n",
    "just say the result? If it music tell the genra of the music,\n",
    "if it is a image tell the genre of the image tell genre of the image\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Science Fiction blended with Cyberpunk. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "message = HumanMessage([\n",
    "                        human_message_text,\n",
    "                        {\n",
    "                            \"type\": \"media\",\n",
    "                            \"file_uri\": _file.uri,\n",
    "                            \"mime_type\": _file.mime_type,\n",
    "                        },])\n",
    "\n",
    "\n",
    "# Invoke the model and print the response\n",
    "response = llm.invoke([message])\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_319/2038500445.py:8: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory()\n",
      "/tmp/ipykernel_319/2038500445.py:11: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use :meth:`~RunnableWithMessageHistory: https://python.langchain.com/v0.2/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html` instead.\n",
      "  chain = ConversationChain(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'res2': {'input': \"What's my name?\", 'history': \"Human: Hi! I'm Jim.\\nAI: Hi Jim, it's great to be chatting with you! I don't have a name, but I'm a large language model, here to assist you with any questions or tasks you might have. What can I do for you today? ðŸ˜Š \\n\", 'response': 'AI: You just told me your name is Jim! ðŸ˜Š  Are you testing my memory? \\n'}}\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains.conversation.base import ConversationChain\n",
    "\n",
    "# Initialize the OpenAI model\n",
    "model = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\")\n",
    "\n",
    "# Initialize the memory\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "# Create the conversation chain\n",
    "chain = ConversationChain(\n",
    "    llm=model,\n",
    "    memory=memory\n",
    ")\n",
    "\n",
    "# Make the first call\n",
    "res1 = chain.invoke({\"input\": \"Hi! I'm Jim.\"})\n",
    "\n",
    "# Make the second call\n",
    "res2 = chain.invoke({\"input\": \"What's my name?\"})\n",
    "\n",
    "# Print the result\n",
    "print({\"res2\": res2}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'res1'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'text'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"Nice to meet you, Jim! I'm an AI, so I don't have a name like you do.  What can I do for you </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">today?  Are you interested in learning about something specific, playing a game, or just chatting?  I'm happy to do</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">whatever you'd like! \\n\"</span>\n",
       "    <span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'memory'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'chat_history'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'The AI introduces itself and asks Jim what it can do for him.  It offers to answer </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">questions, play games, or just chat. \\n'</span>\n",
       "    <span style=\"font-weight: bold\">}</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\n",
       "    \u001b[32m'res1'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "        \u001b[32m'text'\u001b[0m: \u001b[32m\"Nice to meet you, Jim! I'm an AI, so I don't have a name like you do.  What can I do for you \u001b[0m\n",
       "\u001b[32mtoday?  Are you interested in learning about something specific, playing a game, or just chatting?  I'm happy to do\u001b[0m\n",
       "\u001b[32mwhatever you'd like! \\n\"\u001b[0m\n",
       "    \u001b[1m}\u001b[0m,\n",
       "    \u001b[32m'memory'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "        \u001b[32m'chat_history'\u001b[0m: \u001b[32m'The AI introduces itself and asks Jim what it can do for him.  It offers to answer \u001b[0m\n",
       "\u001b[32mquestions, play games, or just chat. \\n'\u001b[0m\n",
       "    \u001b[1m}\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'res2'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'text'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'I do not know your name. I am an AI and do not have access to personal information like names. If </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">you would like to tell me your name, I would be happy to remember it for future conversations. ðŸ˜Š \\n'</span>\n",
       "    <span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'memory'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'chat_history'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"The AI introduces itself and asks Jim what it can do for him.  It offers to answer </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">questions, play games, or just chat.  Jim asks the AI what his name is, and the AI responds that it does not have </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">access to personal information like names. It offers to remember Jim's name if he tells it. \\n\"</span>\n",
       "    <span style=\"font-weight: bold\">}</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\n",
       "    \u001b[32m'res2'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "        \u001b[32m'text'\u001b[0m: \u001b[32m'I do not know your name. I am an AI and do not have access to personal information like names. If \u001b[0m\n",
       "\u001b[32myou would like to tell me your name, I would be happy to remember it for future conversations. ðŸ˜Š \\n'\u001b[0m\n",
       "    \u001b[1m}\u001b[0m,\n",
       "    \u001b[32m'memory'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "        \u001b[32m'chat_history'\u001b[0m: \u001b[32m\"The AI introduces itself and asks Jim what it can do for him.  It offers to answer \u001b[0m\n",
       "\u001b[32mquestions, play games, or just chat.  Jim asks the AI what his name is, and the AI responds that it does not have \u001b[0m\n",
       "\u001b[32maccess to personal information like names. It offers to remember Jim's name if he tells it. \\n\"\u001b[0m\n",
       "    \u001b[1m}\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain.prompts import PromptTemplate\n",
    "from operator import itemgetter\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# Initialize the memory with a summarization model\n",
    "memory = ConversationSummaryMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    llm=ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")\n",
    ",\n",
    "    return_messages=False  # Return string instead of messages\n",
    ")\n",
    "\n",
    "# Initialize the main model for conversation\n",
    "model = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")\n",
    "\n",
    "# Create the prompt template\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"\"\"The following is a friendly conversation between a human and an AI. \\\n",
    "The AI is talkative and provides lots of specific details from its context. \\\n",
    "If the AI does not know the answer to a question, it truthfully says it does not know.\n",
    "\n",
    "Current conversation:\n",
    "{chat_history}\n",
    "Human: {input}\n",
    "AI:\"\"\"\n",
    ")\n",
    "\n",
    "# Define a function to load memory\n",
    "def load_memory(input_dict):\n",
    "    memory_vars = memory.load_memory_variables({})\n",
    "    return memory_vars.get(\"chat_history\", \"\")\n",
    "\n",
    "# Create the chain using the newer approach\n",
    "chain = (\n",
    "    {\n",
    "        \"chat_history\": RunnablePassthrough(load_memory),\n",
    "        \"input\": RunnablePassthrough()\n",
    "    }\n",
    "    | prompt\n",
    "    | model\n",
    ")\n",
    "\n",
    "# First interaction\n",
    "input1 = \"Hi! I'm Jim.\"\n",
    "res1 = chain.invoke(input1)\n",
    "# Save the interaction to memory\n",
    "memory.save_context({\"input\": input1}, {\"output\": res1.content})\n",
    "memory_vars1 = memory.load_memory_variables({})\n",
    "\n",
    "rich.print({\n",
    "    \"res1\": {\"text\": res1.content},\n",
    "    \"memory\": memory_vars1\n",
    "})\n",
    "\n",
    "# Second interaction\n",
    "input2 = \"What's my name?\"\n",
    "res2 = chain.invoke(input2)\n",
    "# Save the interaction to memory\n",
    "memory.save_context({\"input\": input2}, {\"output\": res2.content})\n",
    "memory_vars2 = memory.load_memory_variables({})\n",
    "\n",
    "rich.print({\n",
    "    \"res2\": {\"text\": res2.content},\n",
    "    \"memory\": memory_vars2\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = logging.getLogger()\n",
    "fhandler = logging.FileHandler(filename='mylog.log', mode='a')\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "fhandler.setFormatter(formatter)\n",
    "logger.addHandler(fhandler)\n",
    "logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAI\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain, RetrievalQA\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "import os\n",
    "from typing import Optional, List\n",
    "\n",
    "class AIArtistBot:\n",
    "    def __init__(self, google_api_key: str, knowledge_dir: str):\n",
    "        \"\"\"\n",
    "        Initialize the AI Artist chatbot\n",
    "        \n",
    "        Args:\n",
    "            google_api_key: API key for Google Generative AI\n",
    "            knowledge_dir: Directory containing knowledge base documents\n",
    "        \"\"\"\n",
    "        self.llm = GoogleGenerativeAI(\n",
    "            model=\"gemini-1.5-flash\",\n",
    "            google_api_key=google_api_key,\n",
    "            temperature=0.7\n",
    "        )\n",
    "        \n",
    "        # Initialize embeddings\n",
    "        logging.info(' loading embeddings')\n",
    "        self.embeddings = HuggingFaceEmbeddings(\n",
    "            model_name='paraphrase-albert-small-v2',\n",
    "            model_kwargs={'device': 'cpu'},\n",
    "            encode_kwargs={'normalize_embeddings': True},\n",
    "            cache_folder=\"./.cache/huggingface\"  # Local cache for faster loading\n",
    "        )\n",
    "        \n",
    "        # Load and process knowledge base\n",
    "        logging.info(' creating vector store')\n",
    "        self.vector_store = self._setup_knowledge_base(knowledge_dir)\n",
    "        \n",
    "        # Setup conversation memory\n",
    "        self.memory = ConversationBufferMemory(\n",
    "            memory_key=\"chat_history\",\n",
    "            return_messages=True\n",
    "        )\n",
    "        \n",
    "        # Initialize prompt templates\n",
    "        logging.info(' setting up prompts')\n",
    "        self._setup_prompts()\n",
    "        \n",
    "    def _setup_knowledge_base(self, knowledge_dir: str):\n",
    "        \"\"\"Set up the RAG knowledge base\"\"\"\n",
    "        # Load documents\n",
    "        loader = DirectoryLoader(\n",
    "            knowledge_dir,\n",
    "            glob=\"**/*.txt\",\n",
    "            loader_cls=TextLoader\n",
    "        )\n",
    "        documents = loader.load()\n",
    "        \n",
    "        # Split documents\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=1000,\n",
    "            chunk_overlap=200\n",
    "        )\n",
    "        splits = text_splitter.split_documents(documents)\n",
    "        \n",
    "        # Create vector store\n",
    "        return Chroma.from_documents(\n",
    "            documents=splits,\n",
    "            embedding=self.embeddings\n",
    "        )\n",
    "    \n",
    "    def _setup_prompts(self):\n",
    "        \"\"\"Set up prompt templates for different generation tasks\"\"\"\n",
    "        self.base_prompt = PromptTemplate(\n",
    "            input_variables=[\"context\", \"chat_history\", \"task_type\", \"user_request\"],\n",
    "            template=\"\"\"You are AI Artist, an expert in generating prompts for various multimedia generation tools.\n",
    "            Use the following context and conversation history to generate an appropriate prompt.\n",
    "            \n",
    "            Context: {context}\n",
    "            \n",
    "            Chat History: {chat_history}\n",
    "            \n",
    "            Task Type: {task_type}\n",
    "            User Request: {user_request}\n",
    "            \n",
    "            Generate a detailed prompt that:\n",
    "            1. Incorporates relevant artistic styles and techniques from the context\n",
    "            2. Uses appropriate tool-specific syntax and parameters\n",
    "            3. Maintains consistency with the user's request\n",
    "            4. Includes specific technical details (resolution, aspect ratio, etc.) when relevant\n",
    "            \n",
    "            Generated Prompt:\"\"\"\n",
    "        )\n",
    "    \n",
    "    def generate_prompt(\n",
    "        self,\n",
    "        user_request: str,\n",
    "        task_type: str = \"image\",  # image, video, or music\n",
    "        num_results: int = 1\n",
    "    ) -> List[str]:\n",
    "        \"\"\"\n",
    "        Generate prompts based on user request and task type\n",
    "        \n",
    "        Args:\n",
    "            user_request: User's prompt generation request\n",
    "            task_type: Type of media to generate prompt for\n",
    "            num_results: Number of alternative prompts to generate\n",
    "            \n",
    "        Returns:\n",
    "            List of generated prompts\n",
    "        \"\"\"\n",
    "        # Retrieve relevant context\n",
    "        retriever = self.vector_store.as_retriever(\n",
    "            search_kwargs={\"k\": 3}\n",
    "        )\n",
    "        docs = retriever.get_relevant_documents(user_request)\n",
    "        context = \"\\n\".join([doc.page_content for doc in docs])\n",
    "        \n",
    "        # Create chain\n",
    "        chain = LLMChain(\n",
    "            llm=self.llm,\n",
    "            prompt=self.base_prompt,\n",
    "            memory=self.memory\n",
    "        )\n",
    "        \n",
    "        # Generate prompts\n",
    "        prompts = []\n",
    "        for _ in range(num_results):\n",
    "            response = chain.run(\n",
    "                context=context,\n",
    "                task_type=task_type,\n",
    "                user_request=user_request\n",
    "            )\n",
    "            prompts.append(response.strip())\n",
    "            \n",
    "        return prompts\n",
    "    \n",
    "    def add_knowledge(self, text: str, source_name: str):\n",
    "        \"\"\"\n",
    "        Add new knowledge to the bot's knowledge base\n",
    "        \n",
    "        Args:\n",
    "            text: Text content to add\n",
    "            source_name: Name/identifier for the source\n",
    "        \"\"\"\n",
    "        # Split new content\n",
    "        splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=1000,\n",
    "            chunk_overlap=200\n",
    "        )\n",
    "        splits = splitter.split_text(text)\n",
    "        \n",
    "        # Add to vector store\n",
    "        self.vector_store.add_texts(splits)\n",
    "\n",
    "# Example knowledge base structure for different tools and styles\n",
    "KNOWLEDGE_STRUCTURE = \"\"\"\n",
    "knowledge_base/\n",
    "â”œâ”€â”€ image/\n",
    "â”‚   â”œâ”€â”€ midjourney_styles.txt\n",
    "â”‚   â”œâ”€â”€ stable_diffusion_styles.txt\n",
    "â”‚   â”œâ”€â”€ dall_e_styles.txt\n",
    "â”‚   â””â”€â”€ art_movements.txt\n",
    "â”œâ”€â”€ video/\n",
    "â”‚   â”œâ”€â”€ runway_prompts.txt\n",
    "â”‚   â”œâ”€â”€ gen1_styles.txt\n",
    "â”‚   â””â”€â”€ video_techniques.txt\n",
    "â””â”€â”€ music/\n",
    "    â”œâ”€â”€ mubert_styles.txt\n",
    "    â”œâ”€â”€ soundraw_prompts.txt\n",
    "    â””â”€â”€ music_genres.txt\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Dict, List\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01:51:23 INFO: loading knowledg base\n",
      "01:51:23 INFO: loading ai artist\n",
      "01:51:23 INFO: loading embeddings\n",
      "01:52:38 INFO: creating vector store\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Could not import chromadb python package. Please install it with `pip install chromadb`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/arita/lib/python3.11/site-packages/langchain_community/vectorstores/chroma.py:83\u001b[0m, in \u001b[0;36mChroma.__init__\u001b[0;34m(self, collection_name, embedding_function, persist_directory, client_settings, collection_metadata, client, relevance_score_fn)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 83\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mchromadb\u001b[39;00m\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mchromadb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'chromadb'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 151\u001b[0m\n\u001b[1;32m    148\u001b[0m     demonstrate_knowledge_addition(bot)\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 151\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[27], line 138\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;66;03m# Initialize the bot\u001b[39;00m\n\u001b[1;32m    137\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m loading ai artist\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 138\u001b[0m bot \u001b[38;5;241m=\u001b[39m \u001b[43mAIArtistBot\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgoogle_api_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menviron\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGOOGLE_API_KEY\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mknowledge_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mknowledge_dir\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;66;03m# Run example scenarios\u001b[39;00m\n\u001b[1;32m    144\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m running example scenario\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[26], line 38\u001b[0m, in \u001b[0;36mAIArtistBot.__init__\u001b[0;34m(self, google_api_key, knowledge_dir)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Load and process knowledge base\u001b[39;00m\n\u001b[1;32m     37\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m creating vector store\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 38\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvector_store \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setup_knowledge_base\u001b[49m\u001b[43m(\u001b[49m\u001b[43mknowledge_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Setup conversation memory\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemory \u001b[38;5;241m=\u001b[39m ConversationBufferMemory(\n\u001b[1;32m     42\u001b[0m     memory_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchat_history\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     43\u001b[0m     return_messages\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     44\u001b[0m )\n",
      "Cell \u001b[0;32mIn[26], line 68\u001b[0m, in \u001b[0;36mAIArtistBot._setup_knowledge_base\u001b[0;34m(self, knowledge_dir)\u001b[0m\n\u001b[1;32m     65\u001b[0m splits \u001b[38;5;241m=\u001b[39m text_splitter\u001b[38;5;241m.\u001b[39msplit_documents(documents)\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m# Create vector store\u001b[39;00m\n\u001b[0;32m---> 68\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mChroma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_documents\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/arita/lib/python3.11/site-packages/langchain_community/vectorstores/chroma.py:878\u001b[0m, in \u001b[0;36mChroma.from_documents\u001b[0;34m(cls, documents, embedding, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001b[0m\n\u001b[1;32m    876\u001b[0m texts \u001b[38;5;241m=\u001b[39m [doc\u001b[38;5;241m.\u001b[39mpage_content \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[1;32m    877\u001b[0m metadatas \u001b[38;5;241m=\u001b[39m [doc\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[0;32m--> 878\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_texts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtexts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollection_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpersist_directory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpersist_directory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient_settings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient_settings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    887\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcollection_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollection_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    888\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/arita/lib/python3.11/site-packages/langchain_community/vectorstores/chroma.py:814\u001b[0m, in \u001b[0;36mChroma.from_texts\u001b[0;34m(cls, texts, embedding, metadatas, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001b[0m\n\u001b[1;32m    781\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    782\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_texts\u001b[39m(\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;28mcls\u001b[39m: Type[Chroma],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    793\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    794\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Chroma:\n\u001b[1;32m    795\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Create a Chroma vectorstore from a raw documents.\u001b[39;00m\n\u001b[1;32m    796\u001b[0m \n\u001b[1;32m    797\u001b[0m \u001b[38;5;124;03m    If a persist_directory is specified, the collection will be persisted there.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;124;03m        Chroma: Chroma vectorstore.\u001b[39;00m\n\u001b[1;32m    813\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 814\u001b[0m     chroma_collection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    815\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollection_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    816\u001b[0m \u001b[43m        \u001b[49m\u001b[43membedding_function\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    817\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpersist_directory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpersist_directory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    818\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclient_settings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient_settings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    819\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    820\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcollection_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollection_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    821\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    822\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    823\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    824\u001b[0m         ids \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mstr\u001b[39m(uuid\u001b[38;5;241m.\u001b[39muuid4()) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m texts]\n",
      "File \u001b[0;32m~/anaconda3/envs/arita/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:216\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.finalize.<locals>.warn_if_direct_instance\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    215\u001b[0m     emit_warning()\n\u001b[0;32m--> 216\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/arita/lib/python3.11/site-packages/langchain_community/vectorstores/chroma.py:86\u001b[0m, in \u001b[0;36mChroma.__init__\u001b[0;34m(self, collection_name, embedding_function, persist_directory, client_settings, collection_metadata, client, relevance_score_fn)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mchromadb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[0;32m---> 86\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     87\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not import chromadb python package. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     88\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease install it with `pip install chromadb`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     89\u001b[0m     )\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m client \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client_settings \u001b[38;5;241m=\u001b[39m client_settings\n",
      "\u001b[0;31mImportError\u001b[0m: Could not import chromadb python package. Please install it with `pip install chromadb`."
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import Dict, List\n",
    "from pprint import pprint\n",
    "\n",
    "# Import the AIArtistBot class from previous implementation\n",
    "# (Assuming it's saved in ai_artist.py)\n",
    "\n",
    "def setup_example_knowledge_base():\n",
    "    \"\"\"\n",
    "    Create a simple example knowledge base structure and content for testing\n",
    "    \"\"\"\n",
    "    # Create directory structure\n",
    "    base_dir = \"knowledge_base\"\n",
    "    directories = [\"image\", \"video\", \"music\"]\n",
    "    \n",
    "    for dir_name in directories:\n",
    "        os.makedirs(os.path.join(base_dir, dir_name), exist_ok=True)\n",
    "    \n",
    "    # Example content for Midjourney styles\n",
    "    midjourney_content = \"\"\"\n",
    "    STYLE: Cyberpunk\n",
    "    Description: Futuristic dystopian aesthetic\n",
    "    Parameters: --ar 16:9 --stylize 750 --v 5.2\n",
    "    Example Prompts: cyberpunk night market, neon signs reflecting in puddles\n",
    "    \n",
    "    STYLE: Fantasy\n",
    "    Description: Magical and mythical scenes\n",
    "    Parameters: --ar 1:1 --stylize 1000 --v 5.2\n",
    "    Example Prompts: enchanted forest, magical creatures, glowing particles\n",
    "    \"\"\"\n",
    "    \n",
    "    with open(os.path.join(base_dir, \"image\", \"midjourney_styles.txt\"), \"w\") as f:\n",
    "        f.write(midjourney_content)\n",
    "    \n",
    "    return base_dir\n",
    "\n",
    "def format_prompt_result(prompt: str, task_type: str) -> str:\n",
    "    \"\"\"Format the generated prompt with relevant details\"\"\"\n",
    "    if task_type == \"image\":\n",
    "        return f\"ðŸŽ¨ Image Prompt:\\n{prompt}\\n\"\n",
    "    elif task_type == \"video\":\n",
    "        return f\"ðŸŽ¥ Video Prompt:\\n{prompt}\\n\"\n",
    "    else:  # music\n",
    "        return f\"ðŸŽµ Music Prompt:\\n{prompt}\\n\"\n",
    "\n",
    "def run_example_scenarios(bot: AIArtistBot):\n",
    "    \"\"\"Run various example scenarios to demonstrate bot capabilities\"\"\"\n",
    "    \n",
    "    # Example scenarios to test\n",
    "    scenarios = [\n",
    "        {\n",
    "            \"task_type\": \"image\",\n",
    "            \"requests\": [\n",
    "                \"Create a cyberpunk city scene at night with neon lights\",\n",
    "                \"Generate a magical forest with mythical creatures\",\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"task_type\": \"video\",\n",
    "            \"requests\": [\n",
    "                \"Create a timelapse of a city from day to night\",\n",
    "                \"Generate a slow-motion scene of falling autumn leaves\",\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"task_type\": \"music\",\n",
    "            \"requests\": [\n",
    "                \"Create an ambient track with nature sounds\",\n",
    "                \"Generate an epic orchestral piece for a movie trailer\",\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Process each scenario\n",
    "    for scenario in scenarios:\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Testing {scenario['task_type'].upper()} prompts:\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        for request in scenario['requests']:\n",
    "            print(f\"\\nUser Request: {request}\")\n",
    "            prompts = bot.generate_prompt(\n",
    "                user_request=request,\n",
    "                task_type=scenario['task_type'],\n",
    "                num_results=2\n",
    "            )\n",
    "            \n",
    "            print(\"\\nGenerated Prompts:\")\n",
    "            for i, prompt in enumerate(prompts, 1):\n",
    "                print(f\"\\nVariation {i}:\")\n",
    "                print(format_prompt_result(prompt, scenario['task_type']))\n",
    "\n",
    "def demonstrate_knowledge_addition(bot: AIArtistBot):\n",
    "    \"\"\"Demonstrate how to add new knowledge to the bot\"\"\"\n",
    "    \n",
    "    # Example of adding new style information\n",
    "    new_style_info = \"\"\"\n",
    "    STYLE: Steampunk\n",
    "    Description: Victorian-era science fiction aesthetic\n",
    "    Key Elements:\n",
    "    - Brass and copper materials\n",
    "    - Steam-powered machinery\n",
    "    - Victorian fashion elements\n",
    "    - Mechanical gadgets\n",
    "    Parameters: --ar 3:2 --stylize 850 --v 5.2\n",
    "    Example Prompts:\n",
    "    - \"steampunk inventor's workshop, brass machinery, steam pipes, Victorian details\"\n",
    "    - \"steampunk airship floating through clouds, copper and brass, mechanical wings\"\n",
    "    \"\"\"\n",
    "    \n",
    "    # Add the new knowledge\n",
    "    bot.add_knowledge(\n",
    "        text=new_style_info,\n",
    "        source_name=\"new_style_steampunk\"\n",
    "    )\n",
    "    \n",
    "    # Test the new knowledge\n",
    "    print(\"\\n=== Testing New Knowledge ===\")\n",
    "    prompt = bot.generate_prompt(\n",
    "        user_request=\"Create a steampunk invention workshop scene\",\n",
    "        task_type=\"image\",\n",
    "        num_results=1\n",
    "    )[0]\n",
    "    \n",
    "    print(\"\\nPrompt using new steampunk knowledge:\")\n",
    "    print(format_prompt_result(prompt, \"image\"))\n",
    "\n",
    "def main():\n",
    "    # Setup environment\n",
    "    os.environ[\"GOOGLE_API_KEY\"] = \"your_api_key_here\"  # Replace with your actual API key\n",
    "    \n",
    "    logging.info(' loading knowledg base')\n",
    "    # Create example knowledge base\n",
    "    knowledge_dir = setup_example_knowledge_base()\n",
    "    \n",
    "    # Initialize the bot\n",
    "    logging.info(' loading ai artist')\n",
    "    bot = AIArtistBot(\n",
    "        google_api_key=os.environ[\"GOOGLE_API_KEY\"],\n",
    "        knowledge_dir=knowledge_dir\n",
    "    )\n",
    "    \n",
    "    # Run example scenarios\n",
    "    logging.info(' running example scenario')\n",
    "    run_example_scenarios(bot)\n",
    "    \n",
    "    # Demonstrate knowledge addition\n",
    "    demonstrate_knowledge_addition(bot)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01:30:23 ERROR:hello!\n",
      "01:30:23 DEBUG:This is a debug message\n",
      "01:30:23 INFO:this is an info message\n",
      "01:30:23 WARNING:tbllalfhldfhd, warning.\n"
     ]
    }
   ],
   "source": [
    "logging.error('hello!')\n",
    "logging.debug('This is a debug message')\n",
    "logging.info('this is an info message')\n",
    "logging.warning('tbllalfhldfhd, warning.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "APIRemovedInV1",
     "evalue": "\n\nYou tried to access openai.Image, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAPIRemovedInV1\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m chat \u001b[38;5;241m=\u001b[39m ChatOpenAI(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-4\u001b[39m\u001b[38;5;124m\"\u001b[39m, temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.7\u001b[39m, openai_api_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myour-openai-api-key\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Get the description of the image\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m image_description \u001b[38;5;241m=\u001b[39m \u001b[43mdescribe_image\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdemo/chatbot-deploy/database/22hours-paris-tjzf-videoSixteenByNineJumbo1600.jpg.jpeg\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImage Description:\u001b[39m\u001b[38;5;124m\"\u001b[39m, image_description)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Use ChatOpenAI to refine or use the description\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[5], line 11\u001b[0m, in \u001b[0;36mdescribe_image\u001b[0;34m(image_path)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdescribe_image\u001b[39m(image_path):\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(image_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m image_file:\n\u001b[0;32m---> 11\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[43mopenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpurpose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdescription\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdescription\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/code/arita/.venv_arita/lib/python3.11/site-packages/openai/lib/_old_api.py:39\u001b[0m, in \u001b[0;36mAPIRemovedInV1Proxy.__call__\u001b[0;34m(self, *_args, **_kwargs)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m_args: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_kwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m---> 39\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m APIRemovedInV1(symbol\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_symbol)\n",
      "\u001b[0;31mAPIRemovedInV1\u001b[0m: \n\nYou tried to access openai.Image, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "import openai\n",
    "\n",
    "# Configure OpenAI API Key\n",
    "openai.api_key = \"your-openai-api-key\"\n",
    "\n",
    "# Function to describe an image\n",
    "def describe_image(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        response = openai.Image.create(\n",
    "            file=image_file,\n",
    "            purpose=\"description\"\n",
    "        )\n",
    "    return response['data']['description']\n",
    "\n",
    "# Initialize ChatOpenAI\n",
    "chat = ChatOpenAI(model=\"gpt-4\", temperature=0.7, openai_api_key=\"your-openai-api-key\")\n",
    "\n",
    "# Get the description of the image\n",
    "image_description = describe_image(\"demo/chatbot-deploy/database/22hours-paris-tjzf-videoSixteenByNineJumbo1600.jpg.jpeg\")\n",
    "print(\"Image Description:\", image_description)\n",
    "\n",
    "# Use ChatOpenAI to refine or use the description\n",
    "prompt = ChatPromptTemplate.from_template(\"Provide an analysis of the following image description: {image_description}\")\n",
    "response = chat(prompt.format_prompt(image_description=image_description).to_messages())\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/hoka/code/arita/.venv_arita/bin/python: No module named uv\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "uv pip install transormers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BlipProcessor, BlipForConditionalGeneration\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Load BLIP model and processor\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'transformers'"
     ]
    }
   ],
   "source": [
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "from PIL import Image\n",
    "\n",
    "# Load BLIP model and processor\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "\n",
    "# Load and preprocess image\n",
    "image = Image.open(\"path_to_image.jpg\").convert(\"RGB\")\n",
    "inputs = processor(image, return_tensors=\"pt\")\n",
    "\n",
    "# Generate caption\n",
    "outputs = model.generate(**inputs)\n",
    "caption = processor.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"Generated Caption:\", caption)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The image showcases a scenic view of Paris, capturing its iconic architecture and skyline. In the foreground, there are various traditional Parisian buildings with characteristic rooftops, including dome shapes and prominent chimneys. There are also trees providing greenery, enhancing the urban landscape.\\n\\nIn the background, notable landmarks such as the Eiffel Tower can be seen prominently, along with a classical building that likely serves as a monument or cultural site. The sky is painted with soft pastel colors, indicating either dawn or dusk, adding a warm and serene atmosphere to the scene. Overall, it reflects the charm and beauty of Paris.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "def create_payload(image_url: str) -> dict:\n",
    "    return {\n",
    "        \"model\": \"gpt-4o-mini\",\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": [{\"type\": \"text\", \"text\": \"You are a cool image analyst. Your goal is to describe what is in this image.\"}],\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": \"What is in the image?\"},\n",
    "                    {\"type\": \"image_url\", \"image_url\": {\"url\": image_url}}\n",
    "                ]\n",
    "            }\n",
    "        ],\n",
    "        \"max_tokens\": 500\n",
    "    }\n",
    "\n",
    "def get_image_description(image_url: str, api_key: str) -> str:\n",
    "    payload = create_payload(image_url)\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {api_key}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    response = requests.post('https://api.openai.com/v1/chat/completions', headers=headers, json=payload)\n",
    "    r = response.json()\n",
    "    return r[\"choices\"][0][\"message\"][\"content\"]\n",
    "import requests\n",
    "\n",
    "def upload_file(file_path: str, url: str) -> dict:\n",
    "    \"\"\"Uploads a file to the specified URL and returns the server response.\"\"\"\n",
    "    with open(file_path, 'rb') as f:\n",
    "        files = {'file': f}\n",
    "        response = requests.post(url, files=files)\n",
    "    return response.json()['data']['url'].replace('tmpfiles.org/', 'tmpfiles.org/dl/')\n",
    "\n",
    "# Example usage\n",
    "file_path = 'demo/chatbot-deploy/database/22hours-paris-tjzf-videoSixteenByNineJumbo1600.jpg.jpeg'\n",
    "url = 'https://tmpfiles.org/api/v1/upload'\n",
    "image_url = upload_file(file_path, url)\n",
    "\n",
    "# Print the response from the server\n",
    "print(response)\n",
    "# Example usage\n",
    "api_key = \"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\"\n",
    "description = get_image_description(image_url, api_key)\n",
    "description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(r[\"choices\"][0][\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'status': 'success', 'data': {'url': 'https://tmpfiles.org/19316745/22hours-paris-tjzf-videosixteenbyninejumbo1600.jpg.jpeg'}}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# The path to the file you want to upload\n",
    "file_path = 'demo/chatbot-deploy/database/22hours-paris-tjzf-videoSixteenByNineJumbo1600.jpg.jpeg'\n",
    "\n",
    "# The API endpoint\n",
    "url = 'https://tmpfiles.org/api/v1/upload'\n",
    "\n",
    "# Open the file in binary mode and send the POST request\n",
    "with open(file_path, 'rb') as f:\n",
    "    files = {'file': f}\n",
    "    response = requests.post(url, files=files)\n",
    "\n",
    "# Print the response from the server\n",
    "print(response.json())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://tmpfiles.org/dl/19316745/22hours-paris-tjzf-videosixteenbyninejumbo1600.jpg.jpeg'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arita",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
