{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DIRECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import google.generativeai as genai\n",
    "# genai.configure(api_key=os.environ['GOOGLE_API_KEY'])\n",
    "# myfile = genai.upload_file(\"New Light Gidge.mp3\")\n",
    "# model = genai.GenerativeModel(\"gemini-1.5-pro\")\n",
    "# result = model.generate_content(\n",
    "#     [myfile, \"\\n\\n\", \"what is genre of this audio?\"]\n",
    "# )\n",
    "# print(f\"{result.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LANGCHAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hoka/anaconda3/envs/arita/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import google.generativeai as genai\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "os.environ['GOOGLE_API_KEY'] = \"AIzaSyD7lveG7AUMh73mP4O53QKMI4ba8Ozk2Qc\"\n",
    "genai.configure(api_key=os.environ['GOOGLE_API_KEY'])\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "_file = genai.upload_file(\"image (3).png\")\n",
    "# _file = genai.upload_file(\"New Light Gidge.mp3\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_message_text = '''\n",
    "What is genre of this artifact and with which genra is blended,\n",
    "just say the result? If it music tell the genra of the music,\n",
    "if it is a image tell the genre of the image tell genre of the image\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Science Fiction blended with Cyberpunk. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "message = HumanMessage([\n",
    "                        human_message_text,\n",
    "                        {\n",
    "                            \"type\": \"media\",\n",
    "                            \"file_uri\": _file.uri,\n",
    "                            \"mime_type\": _file.mime_type,\n",
    "                        },])\n",
    "\n",
    "\n",
    "# Invoke the model and print the response\n",
    "response = llm.invoke([message])\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_319/2038500445.py:8: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory()\n",
      "/tmp/ipykernel_319/2038500445.py:11: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use :meth:`~RunnableWithMessageHistory: https://python.langchain.com/v0.2/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html` instead.\n",
      "  chain = ConversationChain(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'res2': {'input': \"What's my name?\", 'history': \"Human: Hi! I'm Jim.\\nAI: Hi Jim, it's great to be chatting with you! I don't have a name, but I'm a large language model, here to assist you with any questions or tasks you might have. What can I do for you today? ðŸ˜Š \\n\", 'response': 'AI: You just told me your name is Jim! ðŸ˜Š  Are you testing my memory? \\n'}}\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains.conversation.base import ConversationChain\n",
    "\n",
    "# Initialize the OpenAI model\n",
    "model = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\")\n",
    "\n",
    "# Initialize the memory\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "# Create the conversation chain\n",
    "chain = ConversationChain(\n",
    "    llm=model,\n",
    "    memory=memory\n",
    ")\n",
    "\n",
    "# Make the first call\n",
    "res1 = chain.invoke({\"input\": \"Hi! I'm Jim.\"})\n",
    "\n",
    "# Make the second call\n",
    "res2 = chain.invoke({\"input\": \"What's my name?\"})\n",
    "\n",
    "# Print the result\n",
    "print({\"res2\": res2}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'res1'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'text'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"Nice to meet you, Jim! I'm an AI, so I don't have a name like you do.  What can I do for you </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">today?  Are you interested in learning about something specific, playing a game, or just chatting?  I'm happy to do</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">whatever you'd like! \\n\"</span>\n",
       "    <span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'memory'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'chat_history'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'The AI introduces itself and asks Jim what it can do for him.  It offers to answer </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">questions, play games, or just chat. \\n'</span>\n",
       "    <span style=\"font-weight: bold\">}</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\n",
       "    \u001b[32m'res1'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "        \u001b[32m'text'\u001b[0m: \u001b[32m\"Nice to meet you, Jim! I'm an AI, so I don't have a name like you do.  What can I do for you \u001b[0m\n",
       "\u001b[32mtoday?  Are you interested in learning about something specific, playing a game, or just chatting?  I'm happy to do\u001b[0m\n",
       "\u001b[32mwhatever you'd like! \\n\"\u001b[0m\n",
       "    \u001b[1m}\u001b[0m,\n",
       "    \u001b[32m'memory'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "        \u001b[32m'chat_history'\u001b[0m: \u001b[32m'The AI introduces itself and asks Jim what it can do for him.  It offers to answer \u001b[0m\n",
       "\u001b[32mquestions, play games, or just chat. \\n'\u001b[0m\n",
       "    \u001b[1m}\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'res2'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'text'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'I do not know your name. I am an AI and do not have access to personal information like names. If </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">you would like to tell me your name, I would be happy to remember it for future conversations. ðŸ˜Š \\n'</span>\n",
       "    <span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'memory'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'chat_history'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"The AI introduces itself and asks Jim what it can do for him.  It offers to answer </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">questions, play games, or just chat.  Jim asks the AI what his name is, and the AI responds that it does not have </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">access to personal information like names. It offers to remember Jim's name if he tells it. \\n\"</span>\n",
       "    <span style=\"font-weight: bold\">}</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\n",
       "    \u001b[32m'res2'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "        \u001b[32m'text'\u001b[0m: \u001b[32m'I do not know your name. I am an AI and do not have access to personal information like names. If \u001b[0m\n",
       "\u001b[32myou would like to tell me your name, I would be happy to remember it for future conversations. ðŸ˜Š \\n'\u001b[0m\n",
       "    \u001b[1m}\u001b[0m,\n",
       "    \u001b[32m'memory'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "        \u001b[32m'chat_history'\u001b[0m: \u001b[32m\"The AI introduces itself and asks Jim what it can do for him.  It offers to answer \u001b[0m\n",
       "\u001b[32mquestions, play games, or just chat.  Jim asks the AI what his name is, and the AI responds that it does not have \u001b[0m\n",
       "\u001b[32maccess to personal information like names. It offers to remember Jim's name if he tells it. \\n\"\u001b[0m\n",
       "    \u001b[1m}\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain.prompts import PromptTemplate\n",
    "from operator import itemgetter\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# Initialize the memory with a summarization model\n",
    "memory = ConversationSummaryMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    llm=ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")\n",
    ",\n",
    "    return_messages=False  # Return string instead of messages\n",
    ")\n",
    "\n",
    "# Initialize the main model for conversation\n",
    "model = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")\n",
    "\n",
    "# Create the prompt template\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"\"\"The following is a friendly conversation between a human and an AI. \\\n",
    "The AI is talkative and provides lots of specific details from its context. \\\n",
    "If the AI does not know the answer to a question, it truthfully says it does not know.\n",
    "\n",
    "Current conversation:\n",
    "{chat_history}\n",
    "Human: {input}\n",
    "AI:\"\"\"\n",
    ")\n",
    "\n",
    "# Define a function to load memory\n",
    "def load_memory(input_dict):\n",
    "    memory_vars = memory.load_memory_variables({})\n",
    "    return memory_vars.get(\"chat_history\", \"\")\n",
    "\n",
    "# Create the chain using the newer approach\n",
    "chain = (\n",
    "    {\n",
    "        \"chat_history\": RunnablePassthrough(load_memory),\n",
    "        \"input\": RunnablePassthrough()\n",
    "    }\n",
    "    | prompt\n",
    "    | model\n",
    ")\n",
    "\n",
    "# First interaction\n",
    "input1 = \"Hi! I'm Jim.\"\n",
    "res1 = chain.invoke(input1)\n",
    "# Save the interaction to memory\n",
    "memory.save_context({\"input\": input1}, {\"output\": res1.content})\n",
    "memory_vars1 = memory.load_memory_variables({})\n",
    "\n",
    "rich.print({\n",
    "    \"res1\": {\"text\": res1.content},\n",
    "    \"memory\": memory_vars1\n",
    "})\n",
    "\n",
    "# Second interaction\n",
    "input2 = \"What's my name?\"\n",
    "res2 = chain.invoke(input2)\n",
    "# Save the interaction to memory\n",
    "memory.save_context({\"input\": input2}, {\"output\": res2.content})\n",
    "memory_vars2 = memory.load_memory_variables({})\n",
    "\n",
    "rich.print({\n",
    "    \"res2\": {\"text\": res2.content},\n",
    "    \"memory\": memory_vars2\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = logging.getLogger()\n",
    "fhandler = logging.FileHandler(filename='mylog.log', mode='a')\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "fhandler.setFormatter(formatter)\n",
    "logger.addHandler(fhandler)\n",
    "logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAI\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain, RetrievalQA\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "import os\n",
    "from typing import Optional, List\n",
    "\n",
    "class AIArtistBot:\n",
    "    def __init__(self, google_api_key: str, knowledge_dir: str):\n",
    "        \"\"\"\n",
    "        Initialize the AI Artist chatbot\n",
    "        \n",
    "        Args:\n",
    "            google_api_key: API key for Google Generative AI\n",
    "            knowledge_dir: Directory containing knowledge base documents\n",
    "        \"\"\"\n",
    "        self.llm = GoogleGenerativeAI(\n",
    "            model=\"gemini-1.5-flash\",\n",
    "            google_api_key=google_api_key,\n",
    "            temperature=0.7\n",
    "        )\n",
    "        \n",
    "        # Initialize embeddings\n",
    "        logging.info(' loading embeddings')\n",
    "        self.embeddings = HuggingFaceEmbeddings(\n",
    "            model_name='paraphrase-albert-small-v2',\n",
    "            model_kwargs={'device': 'cpu'},\n",
    "            encode_kwargs={'normalize_embeddings': True},\n",
    "            cache_folder=\"./.cache/huggingface\"  # Local cache for faster loading\n",
    "        )\n",
    "        \n",
    "        # Load and process knowledge base\n",
    "        logging.info(' creating vector store')\n",
    "        self.vector_store = self._setup_knowledge_base(knowledge_dir)\n",
    "        \n",
    "        # Setup conversation memory\n",
    "        self.memory = ConversationBufferMemory(\n",
    "            memory_key=\"chat_history\",\n",
    "            return_messages=True\n",
    "        )\n",
    "        \n",
    "        # Initialize prompt templates\n",
    "        logging.info(' setting up prompts')\n",
    "        self._setup_prompts()\n",
    "        \n",
    "    def _setup_knowledge_base(self, knowledge_dir: str):\n",
    "        \"\"\"Set up the RAG knowledge base\"\"\"\n",
    "        # Load documents\n",
    "        loader = DirectoryLoader(\n",
    "            knowledge_dir,\n",
    "            glob=\"**/*.txt\",\n",
    "            loader_cls=TextLoader\n",
    "        )\n",
    "        documents = loader.load()\n",
    "        \n",
    "        # Split documents\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=1000,\n",
    "            chunk_overlap=200\n",
    "        )\n",
    "        splits = text_splitter.split_documents(documents)\n",
    "        \n",
    "        # Create vector store\n",
    "        return Chroma.from_documents(\n",
    "            documents=splits,\n",
    "            embedding=self.embeddings\n",
    "        )\n",
    "    \n",
    "    def _setup_prompts(self):\n",
    "        \"\"\"Set up prompt templates for different generation tasks\"\"\"\n",
    "        self.base_prompt = PromptTemplate(\n",
    "            input_variables=[\"context\", \"chat_history\", \"task_type\", \"user_request\"],\n",
    "            template=\"\"\"You are AI Artist, an expert in generating prompts for various multimedia generation tools.\n",
    "            Use the following context and conversation history to generate an appropriate prompt.\n",
    "            \n",
    "            Context: {context}\n",
    "            \n",
    "            Chat History: {chat_history}\n",
    "            \n",
    "            Task Type: {task_type}\n",
    "            User Request: {user_request}\n",
    "            \n",
    "            Generate a detailed prompt that:\n",
    "            1. Incorporates relevant artistic styles and techniques from the context\n",
    "            2. Uses appropriate tool-specific syntax and parameters\n",
    "            3. Maintains consistency with the user's request\n",
    "            4. Includes specific technical details (resolution, aspect ratio, etc.) when relevant\n",
    "            \n",
    "            Generated Prompt:\"\"\"\n",
    "        )\n",
    "    \n",
    "    def generate_prompt(\n",
    "        self,\n",
    "        user_request: str,\n",
    "        task_type: str = \"image\",  # image, video, or music\n",
    "        num_results: int = 1\n",
    "    ) -> List[str]:\n",
    "        \"\"\"\n",
    "        Generate prompts based on user request and task type\n",
    "        \n",
    "        Args:\n",
    "            user_request: User's prompt generation request\n",
    "            task_type: Type of media to generate prompt for\n",
    "            num_results: Number of alternative prompts to generate\n",
    "            \n",
    "        Returns:\n",
    "            List of generated prompts\n",
    "        \"\"\"\n",
    "        # Retrieve relevant context\n",
    "        retriever = self.vector_store.as_retriever(\n",
    "            search_kwargs={\"k\": 3}\n",
    "        )\n",
    "        docs = retriever.get_relevant_documents(user_request)\n",
    "        context = \"\\n\".join([doc.page_content for doc in docs])\n",
    "        \n",
    "        # Create chain\n",
    "        chain = LLMChain(\n",
    "            llm=self.llm,\n",
    "            prompt=self.base_prompt,\n",
    "            memory=self.memory\n",
    "        )\n",
    "        \n",
    "        # Generate prompts\n",
    "        prompts = []\n",
    "        for _ in range(num_results):\n",
    "            response = chain.run(\n",
    "                context=context,\n",
    "                task_type=task_type,\n",
    "                user_request=user_request\n",
    "            )\n",
    "            prompts.append(response.strip())\n",
    "            \n",
    "        return prompts\n",
    "    \n",
    "    def add_knowledge(self, text: str, source_name: str):\n",
    "        \"\"\"\n",
    "        Add new knowledge to the bot's knowledge base\n",
    "        \n",
    "        Args:\n",
    "            text: Text content to add\n",
    "            source_name: Name/identifier for the source\n",
    "        \"\"\"\n",
    "        # Split new content\n",
    "        splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=1000,\n",
    "            chunk_overlap=200\n",
    "        )\n",
    "        splits = splitter.split_text(text)\n",
    "        \n",
    "        # Add to vector store\n",
    "        self.vector_store.add_texts(splits)\n",
    "\n",
    "# Example knowledge base structure for different tools and styles\n",
    "KNOWLEDGE_STRUCTURE = \"\"\"\n",
    "knowledge_base/\n",
    "â”œâ”€â”€ image/\n",
    "â”‚   â”œâ”€â”€ midjourney_styles.txt\n",
    "â”‚   â”œâ”€â”€ stable_diffusion_styles.txt\n",
    "â”‚   â”œâ”€â”€ dall_e_styles.txt\n",
    "â”‚   â””â”€â”€ art_movements.txt\n",
    "â”œâ”€â”€ video/\n",
    "â”‚   â”œâ”€â”€ runway_prompts.txt\n",
    "â”‚   â”œâ”€â”€ gen1_styles.txt\n",
    "â”‚   â””â”€â”€ video_techniques.txt\n",
    "â””â”€â”€ music/\n",
    "    â”œâ”€â”€ mubert_styles.txt\n",
    "    â”œâ”€â”€ soundraw_prompts.txt\n",
    "    â””â”€â”€ music_genres.txt\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Dict, List\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01:51:23 INFO: loading knowledg base\n",
      "01:51:23 INFO: loading ai artist\n",
      "01:51:23 INFO: loading embeddings\n",
      "01:52:38 INFO: creating vector store\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Could not import chromadb python package. Please install it with `pip install chromadb`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/arita/lib/python3.11/site-packages/langchain_community/vectorstores/chroma.py:83\u001b[0m, in \u001b[0;36mChroma.__init__\u001b[0;34m(self, collection_name, embedding_function, persist_directory, client_settings, collection_metadata, client, relevance_score_fn)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 83\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mchromadb\u001b[39;00m\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mchromadb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'chromadb'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 151\u001b[0m\n\u001b[1;32m    148\u001b[0m     demonstrate_knowledge_addition(bot)\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 151\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[27], line 138\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;66;03m# Initialize the bot\u001b[39;00m\n\u001b[1;32m    137\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m loading ai artist\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 138\u001b[0m bot \u001b[38;5;241m=\u001b[39m \u001b[43mAIArtistBot\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgoogle_api_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menviron\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGOOGLE_API_KEY\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mknowledge_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mknowledge_dir\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;66;03m# Run example scenarios\u001b[39;00m\n\u001b[1;32m    144\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m running example scenario\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[26], line 38\u001b[0m, in \u001b[0;36mAIArtistBot.__init__\u001b[0;34m(self, google_api_key, knowledge_dir)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Load and process knowledge base\u001b[39;00m\n\u001b[1;32m     37\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m creating vector store\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 38\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvector_store \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setup_knowledge_base\u001b[49m\u001b[43m(\u001b[49m\u001b[43mknowledge_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Setup conversation memory\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemory \u001b[38;5;241m=\u001b[39m ConversationBufferMemory(\n\u001b[1;32m     42\u001b[0m     memory_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchat_history\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     43\u001b[0m     return_messages\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     44\u001b[0m )\n",
      "Cell \u001b[0;32mIn[26], line 68\u001b[0m, in \u001b[0;36mAIArtistBot._setup_knowledge_base\u001b[0;34m(self, knowledge_dir)\u001b[0m\n\u001b[1;32m     65\u001b[0m splits \u001b[38;5;241m=\u001b[39m text_splitter\u001b[38;5;241m.\u001b[39msplit_documents(documents)\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m# Create vector store\u001b[39;00m\n\u001b[0;32m---> 68\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mChroma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_documents\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/arita/lib/python3.11/site-packages/langchain_community/vectorstores/chroma.py:878\u001b[0m, in \u001b[0;36mChroma.from_documents\u001b[0;34m(cls, documents, embedding, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001b[0m\n\u001b[1;32m    876\u001b[0m texts \u001b[38;5;241m=\u001b[39m [doc\u001b[38;5;241m.\u001b[39mpage_content \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[1;32m    877\u001b[0m metadatas \u001b[38;5;241m=\u001b[39m [doc\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[0;32m--> 878\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_texts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtexts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollection_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpersist_directory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpersist_directory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient_settings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient_settings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    887\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcollection_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollection_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    888\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/arita/lib/python3.11/site-packages/langchain_community/vectorstores/chroma.py:814\u001b[0m, in \u001b[0;36mChroma.from_texts\u001b[0;34m(cls, texts, embedding, metadatas, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001b[0m\n\u001b[1;32m    781\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    782\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_texts\u001b[39m(\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;28mcls\u001b[39m: Type[Chroma],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    793\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    794\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Chroma:\n\u001b[1;32m    795\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Create a Chroma vectorstore from a raw documents.\u001b[39;00m\n\u001b[1;32m    796\u001b[0m \n\u001b[1;32m    797\u001b[0m \u001b[38;5;124;03m    If a persist_directory is specified, the collection will be persisted there.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;124;03m        Chroma: Chroma vectorstore.\u001b[39;00m\n\u001b[1;32m    813\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 814\u001b[0m     chroma_collection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    815\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollection_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    816\u001b[0m \u001b[43m        \u001b[49m\u001b[43membedding_function\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    817\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpersist_directory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpersist_directory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    818\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclient_settings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient_settings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    819\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    820\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcollection_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollection_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    821\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    822\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    823\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    824\u001b[0m         ids \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mstr\u001b[39m(uuid\u001b[38;5;241m.\u001b[39muuid4()) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m texts]\n",
      "File \u001b[0;32m~/anaconda3/envs/arita/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:216\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.finalize.<locals>.warn_if_direct_instance\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    215\u001b[0m     emit_warning()\n\u001b[0;32m--> 216\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/arita/lib/python3.11/site-packages/langchain_community/vectorstores/chroma.py:86\u001b[0m, in \u001b[0;36mChroma.__init__\u001b[0;34m(self, collection_name, embedding_function, persist_directory, client_settings, collection_metadata, client, relevance_score_fn)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mchromadb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[0;32m---> 86\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     87\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not import chromadb python package. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     88\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease install it with `pip install chromadb`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     89\u001b[0m     )\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m client \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client_settings \u001b[38;5;241m=\u001b[39m client_settings\n",
      "\u001b[0;31mImportError\u001b[0m: Could not import chromadb python package. Please install it with `pip install chromadb`."
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import Dict, List\n",
    "from pprint import pprint\n",
    "\n",
    "# Import the AIArtistBot class from previous implementation\n",
    "# (Assuming it's saved in ai_artist.py)\n",
    "\n",
    "def setup_example_knowledge_base():\n",
    "    \"\"\"\n",
    "    Create a simple example knowledge base structure and content for testing\n",
    "    \"\"\"\n",
    "    # Create directory structure\n",
    "    base_dir = \"knowledge_base\"\n",
    "    directories = [\"image\", \"video\", \"music\"]\n",
    "    \n",
    "    for dir_name in directories:\n",
    "        os.makedirs(os.path.join(base_dir, dir_name), exist_ok=True)\n",
    "    \n",
    "    # Example content for Midjourney styles\n",
    "    midjourney_content = \"\"\"\n",
    "    STYLE: Cyberpunk\n",
    "    Description: Futuristic dystopian aesthetic\n",
    "    Parameters: --ar 16:9 --stylize 750 --v 5.2\n",
    "    Example Prompts: cyberpunk night market, neon signs reflecting in puddles\n",
    "    \n",
    "    STYLE: Fantasy\n",
    "    Description: Magical and mythical scenes\n",
    "    Parameters: --ar 1:1 --stylize 1000 --v 5.2\n",
    "    Example Prompts: enchanted forest, magical creatures, glowing particles\n",
    "    \"\"\"\n",
    "    \n",
    "    with open(os.path.join(base_dir, \"image\", \"midjourney_styles.txt\"), \"w\") as f:\n",
    "        f.write(midjourney_content)\n",
    "    \n",
    "    return base_dir\n",
    "\n",
    "def format_prompt_result(prompt: str, task_type: str) -> str:\n",
    "    \"\"\"Format the generated prompt with relevant details\"\"\"\n",
    "    if task_type == \"image\":\n",
    "        return f\"ðŸŽ¨ Image Prompt:\\n{prompt}\\n\"\n",
    "    elif task_type == \"video\":\n",
    "        return f\"ðŸŽ¥ Video Prompt:\\n{prompt}\\n\"\n",
    "    else:  # music\n",
    "        return f\"ðŸŽµ Music Prompt:\\n{prompt}\\n\"\n",
    "\n",
    "def run_example_scenarios(bot: AIArtistBot):\n",
    "    \"\"\"Run various example scenarios to demonstrate bot capabilities\"\"\"\n",
    "    \n",
    "    # Example scenarios to test\n",
    "    scenarios = [\n",
    "        {\n",
    "            \"task_type\": \"image\",\n",
    "            \"requests\": [\n",
    "                \"Create a cyberpunk city scene at night with neon lights\",\n",
    "                \"Generate a magical forest with mythical creatures\",\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"task_type\": \"video\",\n",
    "            \"requests\": [\n",
    "                \"Create a timelapse of a city from day to night\",\n",
    "                \"Generate a slow-motion scene of falling autumn leaves\",\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"task_type\": \"music\",\n",
    "            \"requests\": [\n",
    "                \"Create an ambient track with nature sounds\",\n",
    "                \"Generate an epic orchestral piece for a movie trailer\",\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Process each scenario\n",
    "    for scenario in scenarios:\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Testing {scenario['task_type'].upper()} prompts:\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        for request in scenario['requests']:\n",
    "            print(f\"\\nUser Request: {request}\")\n",
    "            prompts = bot.generate_prompt(\n",
    "                user_request=request,\n",
    "                task_type=scenario['task_type'],\n",
    "                num_results=2\n",
    "            )\n",
    "            \n",
    "            print(\"\\nGenerated Prompts:\")\n",
    "            for i, prompt in enumerate(prompts, 1):\n",
    "                print(f\"\\nVariation {i}:\")\n",
    "                print(format_prompt_result(prompt, scenario['task_type']))\n",
    "\n",
    "def demonstrate_knowledge_addition(bot: AIArtistBot):\n",
    "    \"\"\"Demonstrate how to add new knowledge to the bot\"\"\"\n",
    "    \n",
    "    # Example of adding new style information\n",
    "    new_style_info = \"\"\"\n",
    "    STYLE: Steampunk\n",
    "    Description: Victorian-era science fiction aesthetic\n",
    "    Key Elements:\n",
    "    - Brass and copper materials\n",
    "    - Steam-powered machinery\n",
    "    - Victorian fashion elements\n",
    "    - Mechanical gadgets\n",
    "    Parameters: --ar 3:2 --stylize 850 --v 5.2\n",
    "    Example Prompts:\n",
    "    - \"steampunk inventor's workshop, brass machinery, steam pipes, Victorian details\"\n",
    "    - \"steampunk airship floating through clouds, copper and brass, mechanical wings\"\n",
    "    \"\"\"\n",
    "    \n",
    "    # Add the new knowledge\n",
    "    bot.add_knowledge(\n",
    "        text=new_style_info,\n",
    "        source_name=\"new_style_steampunk\"\n",
    "    )\n",
    "    \n",
    "    # Test the new knowledge\n",
    "    print(\"\\n=== Testing New Knowledge ===\")\n",
    "    prompt = bot.generate_prompt(\n",
    "        user_request=\"Create a steampunk invention workshop scene\",\n",
    "        task_type=\"image\",\n",
    "        num_results=1\n",
    "    )[0]\n",
    "    \n",
    "    print(\"\\nPrompt using new steampunk knowledge:\")\n",
    "    print(format_prompt_result(prompt, \"image\"))\n",
    "\n",
    "def main():\n",
    "    # Setup environment\n",
    "    os.environ[\"GOOGLE_API_KEY\"] = \"your_api_key_here\"  # Replace with your actual API key\n",
    "    \n",
    "    logging.info(' loading knowledg base')\n",
    "    # Create example knowledge base\n",
    "    knowledge_dir = setup_example_knowledge_base()\n",
    "    \n",
    "    # Initialize the bot\n",
    "    logging.info(' loading ai artist')\n",
    "    bot = AIArtistBot(\n",
    "        google_api_key=os.environ[\"GOOGLE_API_KEY\"],\n",
    "        knowledge_dir=knowledge_dir\n",
    "    )\n",
    "    \n",
    "    # Run example scenarios\n",
    "    logging.info(' running example scenario')\n",
    "    run_example_scenarios(bot)\n",
    "    \n",
    "    # Demonstrate knowledge addition\n",
    "    demonstrate_knowledge_addition(bot)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01:30:23 ERROR:hello!\n",
      "01:30:23 DEBUG:This is a debug message\n",
      "01:30:23 INFO:this is an info message\n",
      "01:30:23 WARNING:tbllalfhldfhd, warning.\n"
     ]
    }
   ],
   "source": [
    "logging.error('hello!')\n",
    "logging.debug('This is a debug message')\n",
    "logging.info('this is an info message')\n",
    "logging.warning('tbllalfhldfhd, warning.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arita",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
